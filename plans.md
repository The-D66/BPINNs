# Implementation Plan: ST-APINO (Spatio-Temporal Autoregressive Physics-Informed Neural Operator)

## 1. Objective
Develop a hybrid **Conv1D + Transformer** architecture for solving 1D Saint-Venant equations.
The model learns the evolution operator $\mathcal{G}: U_{[t-W, t]} \to U_{t+1}$.
Key constraint: **During inference, only spatial boundary conditions are provided from external sources. The internal domain evolves purely autoregressively.**

## 2. Architecture: ConvFormer-1D

The model treats the problem as a sequence-to-sequence task where spatial features are extracted first, followed by temporal evolution modeling.

### 2.1 Input/Output
*   **Input Tensor**: $\mathcal{X} \in \mathbb{R}^{B \times T_{win} \times N_x \times C_{in}}$
    *   $T_{win} = 120$ (History window, ~2 hours).
    *   $N_x = 200$ (Spatial grid points).
    *   $C_{in} = 3$: $[h, u, M_{bnd}]$. $M_{bnd}$ is a binary mask (1 at boundaries, 0 inside).
*   **Output Tensor**: $\mathcal{Y} \in \mathbb{R}^{B \times N_x \times 2}$
    *   Predicts $[h_{t+1}, u_{t+1}]$ (or residual $[\Delta h, \Delta u]$).

### 2.2 Model Components

1.  **Time-Distributed Spatial Encoder (Conv1D)**
    *   **Input**: $(B, T_{win}, N_x, 3)$
    *   **Operation**: Apply shared Conv1D layers to each time frame independently.
    *   **Structure**: 3x Residual Blocks (Conv1D, k=5, filters=64).
    *   **Output**: $(B, T_{win}, N_x, 64)$ - "Spatial Latent Sequence".

2.  **Temporal Evolution Module (Transformer)**
    *   **Reshape**: Permute to $(B \times N_x, T_{win}, 64)$. We treat each spatial point's history as a sequence.
    *   **Positional Encoding**: Add learnable/sinusoidal temporal encodings.
    *   **Transformer Encoder**: 2 layers, 4 heads, FFN_dim=128. Attention is applied over the time dimension $T_{win}$.
    *   **Aggregation**: Take the embedding of the last time step (or use a [CLS] token/GlobalPooling) to represent the "Current Temporal State".
    *   **Output**: $(B \times N_x, 64) \to \text{Reshape} \to (B, N_x, 64)$.

3.  **Spatial Decoder (Conv1D)**
    *   **Input**: $(B, N_x, 64)$
    *   **Operation**: 1-2 Conv1D layers to smooth spatially and project to physical dimension.
    *   **Output**: $(B, N_x, 2)$.

## 3. Data Pipeline

### 3.1 Data Loader (`src/setup/window_loader.py`)
*   **Source**: Data will be generated by `pyclaw` through `src/setup/generate_parametric_data.py`.
*   **Processing**:
    *   Normalize data using global statistics.
    *   Create sliding windows of length $T_{win} + 1$.
    *   Split into `Input` ($0 \dots T_{win}-1$) and `Target` ($T_{win}$). 
*   **Batching**: Randomly sample windows from different simulation cases.

### 3.2 PyClaw Integration (`src/setup/generate_parametric_data.py`)
*   **Objective**: Replace the NumPy MacCormack solver with `pyclaw` for high-fidelity simulation.
*   **Implementation Details**:
    1.  **Imports**:
        *   `from clawpack import pyclaw`
        *   `from clawpack import riemann` (specifically `riemann.shallow_roe_with_efix_1D`)
    2.  **New Solver Function**: `solve_saint_venant_pyclaw(...)`
        *   **Domain**: Create `pyclaw.Domain` with `Nx_pyclaw` cells (finer than output grid).
        *   **State**: Initialize `pyclaw.State` with `num_eqn=2` (h, hu) and `num_aux=1` (bathymetry if needed, or handle slope in source).
        *   **Physics**: Set `state.problem_data` for `grav`, `manning`, `slope`.
        *   **Boundary Conditions**:
            *   Implement custom `bc_lower` function for time-dependent Inflow $Q_{in}(t)$.
            *   Implement custom `bc_upper` function for Fixed Level Outflow $h_{out}$.
            *   Assign `solver.bc_lower[0] = pyclaw.BC.custom` and `solver.bc_upper[0] = pyclaw.BC.custom`.
        *   **Source Terms**:
            *   Implement `step_source(solver, state, dt)` to handle Bed Slope $S_0$ and Friction $S_f$.
            *   Use semi-implicit or splitting method for stability.
        *   **Controller**: Use `pyclaw.Controller` to step from $t=0$ to $T_{total}$.
    3.  **Data Extraction & Interpolation**:
        *   The solver runs on `(Nt_pyclaw, Nx_pyclaw)` grid to satisfy CFL.
        *   The output must be interpolated to `(Nt_save, Nx_save)` (e.g., 240x200) using `scipy.interpolate.interp1d`.
        *   Output format: `h_field` (Nt, Nx), `u_field` (Nt, Nx).

## 4. Training Strategy (Teacher Forcing) with Gradient-Weighted Loss

*   **Trainer**: `src/algorithms/Seq2SeqTrainer.py`
*   **Loss Function**:
    *   **Gradient-Weighted Data Loss**: To solve the "lazy prediction" problem, we weight the MSE by the spatial and temporal gradients of the **Ground Truth**.
        $$ W_{i} = 1 + \alpha \left| \frac{\partial U_{GT}}{\partial t} \right| + \beta \left| \frac{\partial U_{GT}}{\partial x} \right| $$
        $$ \mathcal{L}_{Data} = \text{Mean}( W \cdot (\hat{U} - U_{GT})^2 ) $$
        *   **Hyperparameters**: $\alpha = 10.0$, $\beta = 5.0$.
    *   **PDE Loss**: Discrete residual of Saint-Venant equations.
        *   $\partial_t$: Finite difference $(\hat{U}_{t+1} - U_{t}) / dt$.
        *   $\partial_x$: Fixed Conv1D kernel `[-1, 0, 1] / 2dx`.
    *   **Total**: $\mathcal{L} = \mathcal{L}_{Data} + \lambda \mathcal{L}_{PDE}$.

## 5. Inference Strategy & Logic (`src/verify_rollout.py`)

The inference phase simulates a real-world forecasting scenario where only the boundary conditions are known for the future.

### 5.1 Logic Flow
1.  **Initialization**:
    *   Load a Test Case (Ground Truth $U_{GT}$ for comparison).
    *   Initialize `HistoryBuffer` with the first $T_{win}=120$ frames of $U_{GT}$ (Warm-up period).
    *   Initialize `PredictionList` to store results.

2.  **Time-Stepping Loop** (for $t = 120$ to $T_{end}$, e.g., $T_{end}=360$ (4 hours) for a total rollout of 240 steps):
    *   **A. Prepare Input**:
        *   Normalize `HistoryBuffer`.
        *   Append `Mask` channel (1 at $x=0, L$; 0 elsewhere).
        *   Input Shape: $(1, T_{win}, N_x, 3)$.
    *   **B. Model Prediction**:
        *   $\hat{U}_{next} = \text{Model}(\text{HistoryBuffer})$.
        *   Output Shape: $(1, N_x, 2)$.
    *   **C. Boundary Injection (CRITICAL)**:
        *   Fetch physical boundary values for next step: $BC_{in} = U_{GT}[t+1, 0]$, $BC_{out} = U_{GT}[t+1, -1]$.
        *   **Hard Overwrite**:
            *   $\hat{U}_{next}[0, 0, :] = BC_{in}$
            *   $\hat{U}_{next}[0, -1, :] = BC_{out}$
    *   **D. Update State**:
        *   Store $\hat{U}_{next}$ in `PredictionList`.
        *   **FIFO Shift**: Remove oldest frame from `HistoryBuffer`, append $\hat{U}_{next}$ (normalized) to the end.
        *   *Note*: The internal points of `HistoryBuffer` are now purely model-generated.

3.  **Evaluation**:
    *   Compare `PredictionList` vs `GroundTruth[window_size : window_size + rollout_steps]`.

## 6. Validation Metrics & Conservation

### 6.1 Relative L2 Error
$$ \epsilon(t) = \frac{\| \hat{U}_t - U_{GT, t} \|_2}{\| U_{GT, t} \|_2} $$
*   **Target**: $\epsilon(t=4h) < 10\%$.

### 6.2 Mass Conservation Metric
To ensure the model isn't "hallucinating" water, we check the integral balance.

*   **Total Mass**: $M(t) = \sum_{i=0}^{N_x} h_i(t) \Delta x$
*   **Cumulative Flux**: $F(t) = \sum_{\tau=0}^{t} (Q_{in}(\tau) - Q_{out}(\tau)) \Delta t$
    *   $Q_{in}(\tau)$ and $Q_{out}(\tau)$ are derived from the boundary values $h(\tau, 0), u(\tau, 0)$ and $h(\tau, L), u(\tau, L)$ respectively.
*   **Conservation Residual**:
    $$ R_{mass}(t) = M(t) - M(0) - F(t) $$
*   **Relative Conservation Error**:
    $$ E_{cons} = \frac{\max_t |R_{mass}(t)|}{M(0)} $$
*   **Target**: $E_{cons} < 2\%$.

## 7. File Structure Changes

*   **New**: `src/networks/ConvFormer.py` (The model).
*   **New**: `src/setup/window_loader.py` (The data generator).
*   **New**: `src/algorithms/Seq2SeqTrainer.py` (The training loop).
*   **New**: `src/verify_rollout.py` (The inference test logic described in 5.1).
*   **Modify**: `config/autoregressive_sv.json` (Update parameters).
*   **Modify**: `src/setup/generate_parametric_data.py` (Integrate PyClaw solver).

## 8. Hyperparameters

*   `window_size`: 120
*   `rollout_steps`: 240 (4 hours)
*   `d_model`: 64
*   `n_heads`: 4
*   `batch_size`: 16
*   `learning_rate`: 1e-3